{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9464I-uxLiw"
   },
   "source": [
    "# TF IDF implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnV82tg1xLi0"
   },
   "source": [
    "## Test Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T18:01:27.175844Z",
     "start_time": "2020-01-13T18:01:27.172932Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bUsYm9wjxLi1"
   },
   "outputs": [],
   "source": [
    "## SkLearn# Collection of string documents\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLwmFZfKxLi4"
   },
   "source": [
    "## SkLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T18:01:42.352233Z",
     "start_time": "2020-01-13T18:01:42.345575Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Np4dfQOkxLi4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T18:01:43.041458Z",
     "start_time": "2020-01-13T18:01:43.038262Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-7Om8YpYxLi6",
    "outputId": "0a3bd0f5-4424-4400-944f-4482a80bd799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T18:01:44.362006Z",
     "start_time": "2020-01-13T18:01:44.351890Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dTKplK96xLi-",
    "outputId": "53722fa2-6756-4aa0-f179-37b578bb6890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T18:01:45.527468Z",
     "start_time": "2020-01-13T18:01:45.506654Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T18:01:50.327433Z",
     "start_time": "2020-01-13T18:01:50.312135Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-CTiWHygxLjA",
    "outputId": "8d5a9cde-2c29-4afe-f7b4-1547e88dba4f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T18:01:52.972954Z",
     "start_time": "2020-01-13T18:01:52.963975Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bDKEpbA-xLjD",
    "outputId": "87dafd65-5313-443f-8c6e-1b05cc8c2543",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n",
      "  (1, 8)\t0.281088674033753\n",
      "  (1, 6)\t0.281088674033753\n",
      "  (1, 5)\t0.5386476208856763\n",
      "  (1, 3)\t0.281088674033753\n",
      "  (1, 1)\t0.6876235979836938\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (3, 8)\t0.38408524091481483\n",
      "  (3, 6)\t0.38408524091481483\n",
      "  (3, 3)\t0.38408524091481483\n",
      "  (3, 2)\t0.5802858236844359\n",
      "  (3, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "print(skl_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T18:01:57.367212Z",
     "start_time": "2020-01-13T18:01:57.355963Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3QWo34hexLjF",
    "outputId": "cdc04e08-989f-4bdc-dd7f-f1c82a9f90be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
    "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfIwx5LzxLjI"
   },
   "source": [
    "## Custom TF-IDF Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allowed Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T18:02:45.453632Z",
     "start_time": "2020-01-13T18:02:45.436709Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HjuCcJwXxLjJ"
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "# Make sure its well documented and readble with appropriate comments.\n",
    "# Compare your results with the above sklearn tfidf vectorizer\n",
    "# You are not supposed to use any other library apart from the ones given below\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T18:09:05.413106Z",
     "start_time": "2020-01-13T18:09:05.380714Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class personal_tfidf:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "        self.vocab_ = {}\n",
    "        self.tf_ = {}\n",
    "        self.idf_ = {}\n",
    "        self.idf = []\n",
    "\n",
    "# The Fit Function\n",
    "    def fit(self,corpus):\n",
    "        count = 0\n",
    "        word_list = set()\n",
    "# Creating vocab dictionary for each document with key as word and \n",
    "# value as their frequency \n",
    "        for line in corpus:\n",
    "            for word in line.split():\n",
    "                word_list.add(word)\n",
    "        self.vocab_ = {word:count for count,word in enumerate(sorted(list(word_list)))}\n",
    "     \n",
    "    def transform(self,corpus):\n",
    "        count = 1\n",
    "        for line in corpus:\n",
    "            temp = dict(Counter(line.split()))\n",
    "            local_sum = sum(temp.values())\n",
    "# Counting the word occurence in a document\n",
    "            for key , val in temp.items():\n",
    "                if key in self.idf_.keys():\n",
    "                    self.idf_[key] += 1\n",
    "                else:\n",
    "                    self.idf_[key] = 1\n",
    "# Updating the values of Counter to tf from frequency to tf values\n",
    "                temp[key] = round(val/local_sum,8)\n",
    "            self.corpus.append((temp))\n",
    "\n",
    "# Calculating the idf values for each word        \n",
    "        for key in self.idf_.keys():\n",
    "            self.idf_[key] = round(1 + math.log((len(corpus)+1)/(self.idf_[key]+1)),8)\n",
    "\n",
    "# Keeping the idf for the output in different variable       \n",
    "        for key in self.vocab_.keys():\n",
    "            self.idf.append(self.idf_[key])\n",
    "\n",
    "# Calculating the tf-idf values and also row column for the CSR matrix\n",
    "        row = []\n",
    "        value = []\n",
    "        col = []\n",
    "        for index in range(len(self.corpus)):\n",
    "            for key , val in self.corpus[index].items():\n",
    "                value.append(self.idf_[key] * val)\n",
    "                row.append(index)\n",
    "                col.append(self.vocab_[key])\n",
    "                \n",
    "# Normalizing the output created in the CSR matrix\n",
    "        return  normalize(csr_matrix((value,(row,col)),\n",
    "            shape=(len(corpus),\n",
    "            len(self.vocab_.keys()))),norm='l2')\n",
    "    \n",
    "# Function to print the vocab values \n",
    "    def get_feature_names(self):\n",
    "        return list(self.vocab_.keys())\n",
    "        \n",
    "p = personal_tfidf()\n",
    "p.fit(corpus)\n",
    "output = p.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T15:10:28.004388Z",
     "start_time": "2020-01-12T15:10:27.992584Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(p.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T15:10:29.343002Z",
     "start_time": "2020-01-12T15:10:29.331996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073, 1.22314355, 1.51082562, 1.0, 1.91629073, 1.91629073, 1.0, 1.91629073, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(p.idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T15:10:30.546469Z",
     "start_time": "2020-01-12T15:10:30.533468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T15:10:31.746817Z",
     "start_time": "2020-01-12T15:10:31.737897Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.46979138558088085\n",
      "  (0, 2)\t0.5802858228626505\n",
      "  (0, 3)\t0.3840852413282814\n",
      "  (0, 6)\t0.3840852413282814\n",
      "  (0, 8)\t0.3840852413282814\n",
      "  (1, 1)\t0.6876235869144148\n",
      "  (1, 3)\t0.28108867824349915\n",
      "  (1, 5)\t0.5386476284259701\n",
      "  (1, 6)\t0.28108867824349915\n",
      "  (1, 8)\t0.28108867824349915\n",
      "  (2, 0)\t0.5118485126000253\n",
      "  (2, 3)\t0.2671037878474866\n",
      "  (2, 4)\t0.5118485126000253\n",
      "  (2, 6)\t0.2671037878474866\n",
      "  (2, 7)\t0.5118485126000253\n",
      "  (2, 8)\t0.2671037878474866\n",
      "  (3, 1)\t0.46979138558088085\n",
      "  (3, 2)\t0.5802858228626505\n",
      "  (3, 3)\t0.3840852413282814\n",
      "  (3, 6)\t0.3840852413282814\n",
      "  (3, 8)\t0.3840852413282814\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMxBmVZExLjK"
   },
   "source": [
    "## Custom TF-IDF with max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the word corpus provided for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T18:11:54.662542Z",
     "start_time": "2020-01-13T18:11:54.649568Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NHxPLlwNxLjL",
    "outputId": "9abd8e08-0e24-4975-9a13-4d3636d60323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "# Below is the code to load the cleaned_strings pickle file provided\n",
    "# Here corpus is of list type\n",
    "\n",
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max_Feature TF-IDF Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T18:51:29.595005Z",
     "start_time": "2020-01-14T18:51:29.574600Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZULfoOIdxLjQ"
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "# Try not to hardcode any values.\n",
    "# Make sure its well documented and readble with appropriate comments.\n",
    "\n",
    "\n",
    "class max_idf_tfidf():\n",
    "    \n",
    "    \n",
    "    def __init__(self,max_features=50):\n",
    "        self.corpus = []\n",
    "        self.vocab_ = {}\n",
    "        self.tf_ = {}\n",
    "        self.idf_ = {}\n",
    "        self.idf = []\n",
    "        self.max_features = max_features\n",
    "        \n",
    "    def fit(self,corpus):\n",
    "        count = 0\n",
    "# Creatinng a list of dictionaries for each document containg word and it's count\n",
    "        word_list = set()\n",
    "        for line in corpus:\n",
    "            temp = dict(Counter(line.split()))\n",
    "            for word in line.split():\n",
    "                word_list.add(word)\n",
    "# Counting the word occurence in a document            \n",
    "            for key , val in temp.items():\n",
    "                if key in self.idf_.keys():\n",
    "                    self.idf_[key] += 1\n",
    "                else:\n",
    "                    self.idf_[key] = 1\n",
    "# Calculating the idf values\n",
    "        for key in self.idf_.keys():\n",
    "            self.idf_[key] = round(1 + math.log((len(corpus)+1)/(self.idf_[key]+1)),8)\n",
    "            \n",
    "# Now keeping the top 50 idf values only\n",
    "\n",
    "# Sorting on values descending\n",
    "        self.idf_ = dict(sorted(self.idf_.items(),key = lambda x: (x[1],x[0]),reverse=True))\n",
    "\n",
    "# Creating the original vocab dictionaries\n",
    "        self.vocab = {word:count for count, word in enumerate(sorted(list(word_list))) }\n",
    "    \n",
    "# Creating the vocab with top 50 words          \n",
    "        for key ,val in self.vocab.items():\n",
    "            if key in list(self.idf_.keys())[:self.max_features]:\n",
    "                self.vocab_[key] = val\n",
    "                \n",
    "# Creating the final IDF Values dictionary for top 50 idf values        \n",
    "        for key in self.vocab_.keys():\n",
    "            if key in list(self.idf_.keys())[:self.max_features]:\n",
    "                self.idf.append(self.idf_[key])\n",
    "        \n",
    "     \n",
    "    def transform(self,corpus):\n",
    "        count = 1\n",
    "        \n",
    "# Creating the tf ddictionary as list of dictionaries where each dictionary \n",
    "# is the tf of each document\n",
    "        for line in corpus:\n",
    "            temp = dict(Counter(line.split()))\n",
    "            local_sum = sum(temp.values())\n",
    "            \n",
    "# Updating the values of Counter to tf for\n",
    "            for key , val in temp.items():\n",
    "                temp[key] = round(val/local_sum,8)\n",
    "    \n",
    "# Creating main corpus with the top 50 words only\n",
    "            list_keys = list(temp.keys())\n",
    "            for key in list_keys:\n",
    "                if key not in self.vocab_.keys():\n",
    "                    temp.pop(key,None)\n",
    "            if len(temp.keys()) > 0:\n",
    "                self.corpus.append((temp))\n",
    "        \n",
    "        row = []\n",
    "        value = []\n",
    "        col = []\n",
    "        for index in range(len(self.corpus)):\n",
    "            for key , val in self.corpus[index].items():\n",
    "                value.append(self.idf_[key] * val)\n",
    "                row.append(index)\n",
    "                col.append(self.vocab_[key])\n",
    "#       Normalizing the output\n",
    "        return  normalize(csr_matrix((value,(row,col)),\n",
    "            shape=(len(self.corpus),\n",
    "            len(self.vocab.keys()))),norm='l2')\n",
    "    \n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return list(self.vocab_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T18:53:15.657640Z",
     "start_time": "2020-01-14T18:53:15.463460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weariness', 'weaving', 'website', 'wedding', 'weight', 'welsh', 'went', 'whenever', 'whine', 'whites', 'whoever', 'wide', 'widmark', 'wife', 'wih', 'wild', 'william', 'willie', 'wily', 'win', 'wise', 'within', 'witticisms', 'woa', 'wondered', 'wong', 'wont', 'woo', 'worked', 'worry', 'worthless', 'worthwhile', 'worthy', 'wouldnt', 'woven', 'wow', 'wrap', 'writers', 'wrote', 'x', 'yardley', 'yawn', 'yelps', 'younger', 'youthful', 'youtube', 'yun', 'z', 'zillion', 'zombiez']\n",
      "[6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "p = max_idf_tfidf(max_features=50)\n",
    "p.fit(corpus)\n",
    "print(p.get_feature_names())\n",
    "output = p.transform(corpus)\n",
    "print(p.idf)\n",
    "print(output[0].toarray())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3_Instructions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "318px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
